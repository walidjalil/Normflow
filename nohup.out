Initializing dataset...
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
6900
7000
7100
7200
7300
7400
7500
7600
7700
7800
7900
8000
8100
8200
8300
8400
8500
8600
8700
8800
8900
9000
9100
9200
9300
9400
9500
9600
9700
9800
9900
10000
10100
10200
10300
10400
10500
10600
10700
10800
10900
11000
11100
11200
11300
11400
11500
11600
11700
11800
11900
12000
12100
12200
12300
12400
12500
12600
12700
12800
12900
13000
13100
13200
13300
13400
13500
13600
13700
13800
13900
14000
14100
14200
14300
14400
14500
14600
14700
14800
14900
15000
15100
15200
15300
15400
15500
15600
15700
15800
15900
16000
16100
16200
16300
16400
16500
16600
16700
16800
16900
17000
17100
17200
17300
17400
17500
17600
17700
17800
17900
18000
18100
18200
18300
18400
18500
18600
18700
18800
18900
19000
19100
19200
19300
19400
19500
19600
19700
19800
19900
20000
20100
20200
20300
20400
20500
20600
20700
20800
20900
21000
21100
21200
21300
21400
21500
21600
21700
21800
21900
22000
22100
22200
22300
22400
22500
22600
22700
22800
22900
23000
23100
23200
23300
23400
23500
23600
23700
23800
23900
24000
24100
24200
24300
24400
24500
24600
24700
24800
24900
25000
25100
25200
25300
25400
25500
25600
25700
25800
25900
26000
26100
26200
26300
26400
26500
26600
26700
26800
26900
27000
27100
27200
27300
27400
27500
27600
27700
27800
27900
28000
28100
28200
28300
28400
28500
28600
28700
28800
28900
29000
29100
29200
29300
29400
29500
29600
29700
29800
29900
30000
30100
30200
30300
30400
30500
30600
30700
30800
30900
31000
31100
31200
31300
31400
31500
31600
31700
31800
31900
32000
32100
32200
32300
32400
32500
32600
32700
32800
32900
33000
33100
33200
33300
33400
33500
33600
33700
33800
33900
34000
34100
34200
34300
34400
34500
34600
34700
34800
34900
35000
35100
35200
35300
35400
35500
35600
35700
35800
35900
36000
36100
36200
36300
36400
36500
36600
36700
36800
36900
37000
37100
37200
37300
37400
37500
37600
37700
37800
37900
38000
38100
38200
38300
38400
38500
38600
38700
38800
38900
39000
39100
39200
39300
39400
39500
39600
39700
39800
39900
40000
40100
Dataset initialized
 
Beginning training now:
 
/home/walid_abduljalil/Normflow/loss.py:12: UserWarning: Using a target size (torch.Size([1, 1, 160, 80])) that is different to the input size (torch.Size([160, 80, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  recon_loss = torch.nn.functional.mse_loss(reconstructions, gt_images)
 
Loss:  0.02612709254026413
-----------------------------
 
Loss:  138.7127227783203
-----------------------------
 
Loss:  0.055921610444784164
-----------------------------
 
Loss:  2.1445178985595703
-----------------------------
 
Loss:  0.31061699986457825
-----------------------------
Initializing dataset...
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
6900
7000
7100
7200
7300
7400
7500
7600
7700
7800
7900
8000
8100
8200
8300
8400
8500
8600
8700
8800
8900
9000
9100
9200
9300
9400
9500
9600
9700
9800
9900
10000
10100
10200
10300
10400
10500
10600
10700
10800
10900
11000
11100
11200
11300
11400
11500
11600
11700
11800
11900
12000
12100
12200
12300
12400
12500
12600
12700
12800
12900
13000
13100
13200
13300
13400
13500
13600
13700
13800
13900
14000
14100
14200
14300
14400
14500
14600
14700
14800
14900
15000
15100
15200
15300
15400
15500
15600
15700
15800
15900
16000
16100
16200
16300
16400
16500
16600
16700
16800
16900
17000
17100
17200
17300
17400
17500
17600
17700
17800
17900
18000
18100
18200
18300
18400
18500
18600
18700
18800
18900
19000
19100
19200
19300
19400
19500
19600
19700
19800
19900
20000
20100
20200
20300
20400
20500
20600
20700
20800
20900
21000
21100
21200
21300
21400
21500
21600
21700
21800
21900
22000
22100
22200
22300
22400
22500
22600
22700
22800
22900
23000
23100
23200
23300
23400
23500
23600
23700
23800
23900
24000
24100
24200
24300
24400
24500
24600
24700
24800
24900
25000
25100
25200
25300
25400
25500
25600
25700
25800
25900
26000
26100
26200
26300
26400
26500
26600
26700
26800
26900
27000
27100
27200
27300
27400
27500
27600
27700
27800
27900
28000
28100
28200
28300
28400
28500
28600
28700
28800
28900
29000
29100
29200
29300
29400
29500
29600
29700
29800
29900
30000
30100
30200
30300
30400
30500
30600
30700
30800
30900
31000
31100
31200
31300
31400
31500
31600
31700
31800
31900
32000
32100
32200
32300
32400
32500
32600
32700
32800
32900
33000
33100
33200
33300
33400
33500
33600
33700
33800
33900
34000
34100
34200
34300
34400
34500
34600
34700
34800
34900
35000
35100
35200
35300
35400
35500
35600
35700
35800
35900
36000
36100
36200
36300
36400
36500
36600
36700
36800
36900
37000
37100
37200
37300
37400
37500
37600
37700
37800
37900
38000
38100
38200
38300
38400
38500
38600
38700
38800
38900
39000
39100
39200
39300
39400
39500
39600
39700
39800
39900
40000
40100
Dataset initialized
 
Beginning training now:
 
/home/walid_abduljalil/Normflow/loss.py:12: UserWarning: Using a target size (torch.Size([1, 1, 160, 80])) that is different to the input size (torch.Size([160, 80, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  recon_loss = torch.nn.functional.mse_loss(reconstructions, gt_images)
 
Loss:  0.027607552707195282
-----------------------------
 
Loss:  135.32310485839844
-----------------------------
 
Loss:  0.07409576326608658
-----------------------------
 
Loss:  1.2212326526641846
-----------------------------
 
Loss:  0.37301233410835266
-----------------------------
 
Loss:  0.21247951686382294
-----------------------------
 
Loss:  0.16715729236602783
-----------------------------
 
Loss:  0.20928019285202026
-----------------------------
 
Loss:  0.26262831687927246
-----------------------------
 
Loss:  0.2782343924045563
-----------------------------
Initializing dataset...
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
6900
7000
7100
7200
7300
7400
7500
7600
7700
7800
7900
8000
8100
8200
8300
8400
8500
8600
8700
8800
8900
9000
9100
9200
9300
9400
9500
9600
9700
9800
9900
10000
10100
10200
10300
10400
10500
10600
10700
10800
10900
11000
11100
11200
11300
11400
11500
11600
11700
11800
11900
12000
12100
12200
12300
12400
12500
12600
12700
12800
12900
13000
13100
13200
13300
13400
13500
13600
13700
13800
13900
14000
14100
14200
14300
14400
14500
14600
14700
14800
14900
15000
15100
15200
15300
15400
15500
15600
15700
15800
15900
16000
16100
16200
16300
16400
16500
16600
16700
16800
16900
17000
17100
17200
17300
17400
17500
17600
17700
17800
17900
18000
18100
18200
18300
18400
18500
18600
18700
18800
18900
19000
19100
19200
19300
19400
19500
19600
19700
19800
19900
20000
20100
20200
20300
20400
20500
20600
20700
20800
20900
21000
21100
21200
21300
21400
21500
21600
21700
21800
21900
22000
22100
22200
22300
22400
22500
22600
22700
22800
22900
23000
23100
23200
23300
23400
23500
23600
23700
23800
23900
24000
24100
24200
24300
24400
24500
24600
24700
24800
24900
25000
25100
25200
25300
25400
25500
25600
25700
25800
25900
26000
26100
26200
26300
26400
26500
26600
26700
26800
26900
27000
27100
27200
27300
27400
27500
27600
27700
27800
27900
28000
28100
28200
28300
28400
28500
28600
28700
28800
28900
29000
29100
29200
29300
29400
29500
29600
29700
29800
29900
30000
30100
30200
30300
30400
30500
30600
30700
30800
30900
31000
31100
31200
31300
31400
31500
31600
31700
31800
31900
32000
32100
32200
32300
32400
32500
32600
32700
32800
32900
33000
33100
33200
33300
33400
33500
33600
33700
33800
33900
34000
34100
34200
34300
34400
34500
34600
34700
34800
34900
35000
35100
35200
35300
35400
35500
35600
35700
35800
35900
36000
36100
36200
36300
36400
36500
36600
36700
36800
36900
37000
37100
37200
37300
37400
37500
37600
37700
37800
37900
38000
38100
38200
38300
38400
38500
38600
38700
38800
38900
39000
39100
39200
39300
39400
39500
39600
39700
39800
39900
40000
40100
Dataset initialized
 
Beginning training now:
 
/home/walid_abduljalil/Normflow/loss.py:12: UserWarning: Using a target size (torch.Size([1, 1, 160, 80])) that is different to the input size (torch.Size([160, 80, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  recon_loss = torch.nn.functional.mse_loss(reconstructions, gt_images)
 
Loss:  0.027971139177680016
-----------------------------
 
Loss:  180.46742248535156
-----------------------------
 
Loss:  0.0820201113820076
-----------------------------
 
Loss:  0.979119598865509
-----------------------------
 
Loss:  0.34280842542648315
-----------------------------
 
Loss:  0.23511646687984467
-----------------------------
 
Loss:  0.17875678837299347
-----------------------------
 
Loss:  0.2050846815109253
-----------------------------
 
Loss:  0.25797632336616516
-----------------------------
 
Loss:  0.28424596786499023
-----------------------------
 
Loss:  0.2814710736274719
-----------------------------
 
Loss:  0.2634194791316986
-----------------------------
 
Loss:  0.24695414304733276
-----------------------------
 
Loss:  0.239010289311409
-----------------------------
 
Loss:  0.23904013633728027
-----------------------------
 
Loss:  0.24343471229076385
-----------------------------
 
Loss:  0.2483447641134262
-----------------------------
 
Loss:  0.2513030171394348
-----------------------------
 
Loss:  0.25147175788879395
-----------------------------
 
Loss:  0.24923455715179443
-----------------------------
 
Loss:  0.24550138413906097
-----------------------------
 
Loss:  0.24118715524673462
-----------------------------
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/site-packages/torch/serialization.py", line 372, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/opt/conda/lib/python3.7/site-packages/torch/serialization.py", line 493, in _save
    zip_file.write_record(name, buf_value, len(buf_value))
OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "training.py", line 76, in <module>
    }, path)
  File "/opt/conda/lib/python3.7/site-packages/torch/serialization.py", line 373, in save
    return
  File "/opt/conda/lib/python3.7/site-packages/torch/serialization.py", line 259, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:274] . unexpected pos 57745408 vs 57745296
terminate called after throwing an instance of 'c10::Error'
  what():  [enforce fail at inline_container.cc:274] . unexpected pos 57745408 vs 57745296
frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x47 (0x7f5a45f0c6a7 in /opt/conda/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1c99500 (0x7f5a85273500 in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x1c956d3 (0x7f5a8526f6d3 in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::string const&, void const*, unsigned long, bool) + 0xa9 (0x7f5a85274609 in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #4: caffe2::serialize::PyTorchStreamWriter::writeEndOfFile() + 0xe1 (0x7f5a85275141 in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #5: caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter() + 0x115 (0x7f5a85275935 in /opt/conda/lib/python3.7/site-pInitializing dataset...
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
6900
7000
7100
7200
7300
7400
7500
7600
7700
7800
7900
8000
8100
8200
8300
8400
8500
8600
8700
8800
8900
9000
9100
9200
9300
9400
9500
9600
9700
9800
9900
10000
10100
10200
10300
10400
10500
10600
10700
10800
10900
11000
11100
11200
11300
11400
11500
11600
11700
11800
11900
12000
12100
12200
12300
12400
12500
12600
12700
12800
12900
13000
13100
13200
13300
13400
13500
13600
13700
13800
13900
14000
14100
14200
14300
14400
14500
14600
14700
14800
14900
15000
15100
15200
15300
15400
15500
15600
15700
15800
15900
16000
16100
16200
16300
16400
16500
16600
16700
16800
16900
17000
17100
17200
17300
17400
17500
17600
17700
17800
17900
18000
18100
18200
18300
18400
18500
18600
18700
18800
18900
19000
19100
19200
19300
19400
19500
19600
19700
19800
19900
20000
20100
20200
20300
20400
20500
20600
20700
20800
20900
21000
21100
21200
21300
21400
21500
21600
21700
21800
21900
22000
22100
22200
22300
22400
22500
22600
22700
22800
22900
23000
23100
23200
23300
23400
23500
23600
23700
23800
23900
24000
24100
24200
24300
24400
24500
24600
24700
24800
24900
25000
25100
25200
25300
25400
25500
25600
25700
25800
25900
26000
26100
26200
26300
26400
26500
26600
26700
26800
26900
27000
27100
27200
27300
27400
27500
27600
27700
27800
27900
28000
28100
28200
28300
28400
28500
28600
28700
28800
28900
29000
29100
29200
29300
29400
29500
29600
29700
29800
29900
30000
30100
30200
30300
30400
30500
30600
30700
30800
30900
31000
31100
31200
31300
31400
31500
31600
31700
31800
31900
32000
32100
32200
32300
32400
32500
32600
32700
32800
32900
33000
33100
33200
33300
33400
33500
33600
33700
33800
33900
34000
34100
34200
34300
34400
34500
34600
34700
34800
34900
35000
35100
35200
35300
35400
35500
35600
35700
35800
35900
36000
36100
36200
36300
36400
36500
36600
36700
36800
36900
37000
37100
37200
37300
37400
37500
37600
37700
37800
37900
38000
38100
38200
38300
38400
38500
38600
38700
38800
38900
39000
39100
39200
39300
39400
39500
39600
39700
39800
39900
40000
40100
Dataset initialized
 
Beginning training now:
 
/home/walid_abduljalil/Normflow/loss.py:12: UserWarning: Using a target size (torch.Size([1, 1, 160, 80])) that is different to the input size (torch.Size([160, 80, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  recon_loss = torch.nn.functional.mse_loss(reconstructions, gt_images)
 
Loss:  0.025197621434926987
-----------------------------
 
Loss:  171.96998596191406
-----------------------------
 
Loss:  0.07143598794937134
-----------------------------
 
Loss:  1.010702133178711
-----------------------------
 
Loss:  0.41754719614982605
-----------------------------
 
Loss:  0.246056467294693
-----------------------------
 
Loss:  0.18974754214286804
-----------------------------
 
Loss:  0.21488505601882935
-----------------------------
 
Loss:  0.2715425193309784
-----------------------------
 
Loss:  0.30032581090927124
-----------------------------
 
Loss:  0.2971603572368622
-----------------------------
 
Loss:  0.28026989102363586
-----------------------------
 
Loss:  0.26373839378356934
-----------------------------
 
Loss:  0.2553260326385498
-----------------------------
 
Loss:  0.25553005933761597
-----------------------------
 
Loss:  0.260307252407074
-----------------------------
 
Loss:  0.2654995918273926
-----------------------------
 
Loss:  0.2685185968875885
-----------------------------
 
Loss:  0.2685793340206146
-----------------------------
 
Loss:  0.2661280035972595
-----------------------------
 
Loss:  0.26211968064308167
-----------------------------
 
Loss:  0.25750330090522766
-----------------------------
 
Loss:  0.2529699504375458
-----------------------------
 
Loss:  0.2489463984966278
-----------------------------
 
Loss:  0.24559634923934937
-----------------------------
 
Loss:  0.2429232895374298
-----------------------------
 
Loss:  0.24084623157978058
-----------------------------
 
Loss:  0.23921950161457062
-----------------------------
 
Loss:  0.2379246950149536
-----------------------------
 
Loss:  0.23684479296207428
-----------------------------
 
Loss:  0.23592115938663483
-----------------------------
 
Loss:  0.23507389426231384
-----------------------------
 
Loss:  0.23428510129451752
-----------------------------
 
Loss:  0.23354652523994446
-----------------------------
 
Loss:  0.23282794654369354
-----------------------------
 
Loss:  0.23214948177337646
-----------------------------
 
Loss:  0.2314939647912979
-----------------------------
 
Loss:  0.2308751344680786
-----------------------------
 
Loss:  0.23029577732086182
-----------------------------
 
Loss:  0.2297498732805252
-----------------------------
 
Loss:  0.22924169898033142
-----------------------------
 
Loss:  0.22877982258796692
-----------------------------
 
Loss:  0.22834408283233643
-----------------------------
 
Loss:  0.22794565558433533
-----------------------------
 
Loss:  0.22758416831493378
-----------------------------
 
Loss:  0.22725091874599457
-----------------------------
 
Loss:  0.22694401443004608
-----------------------------
 
Loss:  0.22667282819747925
-----------------------------
 
Loss:  0.22642835974693298
-----------------------------
 
Loss:  0.22620107233524323
-----------------------------
Initializing dataset...
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
6900
7000
7100
7200
7300
7400
7500
7600
7700
7800
7900
8000
8100
8200
8300
8400
8500
8600
8700
8800
8900
9000
9100
9200
9300
9400
9500
9600
9700
9800
9900
10000
10100
10200
10300
10400
10500
10600
10700
10800
10900
11000
11100
11200
11300
11400
11500
11600
11700
11800
11900
12000
12100
12200
12300
12400
12500
12600
12700
12800
12900
13000
13100
13200
13300
13400
13500
13600
13700
13800
13900
14000
14100
14200
14300
14400
14500
14600
14700
14800
14900
15000
15100
15200
15300
15400
15500
15600
15700
15800
15900
16000
16100
16200
16300
16400
16500
16600
16700
16800
16900
17000
17100
17200
17300
17400
17500
17600
17700
17800
17900
18000
18100
18200
18300
18400
18500
18600
18700
18800
18900
19000
19100
19200
19300
19400
19500
19600
19700
19800
19900
20000
20100
20200
20300
20400
20500
20600
20700
20800
20900
21000
21100
21200
21300
21400
21500
21600
21700
21800
21900
22000
22100
22200
22300
22400
22500
22600
22700
22800
22900
23000
23100
23200
23300
23400
23500
23600
23700
23800
23900
24000
24100
24200
24300
24400
24500
24600
24700
24800
24900
25000
25100
25200
25300
25400
25500
25600
25700
25800
25900
26000
26100
26200
26300
26400
26500
26600
26700
26800
26900
27000
27100
27200
27300
27400
27500
27600
27700
27800
27900
28000
28100
28200
28300
28400
28500
28600
28700
28800
28900
29000
29100
29200
29300
29400
29500
29600
29700
29800
29900
30000
30100
30200
30300
30400
30500
30600
30700
30800
30900
31000
31100
31200
31300
31400
31500
31600
31700
31800
31900
32000
32100
32200
32300
32400
32500
32600
32700
32800
32900
33000
33100
33200
33300
33400
33500
33600
33700
33800
33900
34000
34100
34200
34300
34400
34500
34600
34700
34800
34900
35000
35100
35200
35300
35400
35500
35600
35700
35800
35900
36000
36100
36200
36300
36400
36500
36600
36700
36800
36900
37000
37100
37200
37300
37400
37500
37600
37700
37800
37900
38000
38100
38200
38300
38400
38500
38600
38700
38800
38900
39000
39100
39200
39300
39400
39500
39600
39700
39800
39900
40000
40100
Dataset initialized
 
Beginning training now:
 
/home/walid_abduljalil/Normflow/loss.py:12: UserWarning: Using a target size (torch.Size([1, 1, 160, 80])) that is different to the input size (torch.Size([160, 80, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  recon_loss = torch.nn.functional.mse_loss(reconstructions, gt_images)
 
Loss:  2.4383943527936935
-----------------------------
 
Loss:  19186.851501464844
-----------------------------
 
Loss:  6.191927194595337
-----------------------------
 
Loss:  165.0600552558899
-----------------------------
 
Loss:  35.93445420265198
-----------------------------
 
Loss:  19.61929500102997
-----------------------------
 
Loss:  17.201176285743713
-----------------------------
 
Loss:  21.338266134262085
-----------------------------
 
Loss:  26.859736442565918
-----------------------------
 
Loss:  29.155179858207703
-----------------------------
 
Loss:  28.403326869010925
-----------------------------
 
Loss:  26.559197902679443
-----------------------------
 
Loss:  25.09331703186035
-----------------------------
 
Loss:  24.55136626958847
-----------------------------
 
Loss:  24.75026100873947
-----------------------------
 
Loss:  25.290825963020325
-----------------------------
 
Loss:  25.797805190086365
-----------------------------
 
Loss:  26.047593355178833
-----------------------------
 
Loss:  25.992336869239807
-----------------------------
 
Loss:  25.7053405046463
-----------------------------
 
Loss:  25.29783844947815
-----------------------------
 
Loss:  24.868740141391754
-----------------------------
 
Loss:  24.4820237159729
-----------------------------
 
Loss:  24.169471859931946
-----------------------------
 
Loss:  23.93120527267456
-----------------------------
 
Loss:  23.760344088077545
-----------------------------
 
Loss:  23.635704815387726
-----------------------------
 
Loss:  23.54319542646408
-----------------------------
 
Loss:  23.471610248088837
-----------------------------
 
Loss:  23.409223556518555
-----------------------------
 
Loss:  23.352977633476257
-----------------------------
 
Loss:  23.29796999692917
-----------------------------
 
Loss:  23.24363887310028
-----------------------------
 
Loss:  23.18897694349289
-----------------------------
 
Loss:  23.137353360652924
-----------------------------
 
Loss:  23.084498941898346
-----------------------------
 
Loss:  23.03457409143448
-----------------------------
 
Loss:  22.987645864486694
-----------------------------
 
Loss:  22.942401468753815
-----------------------------
 
Loss:  22.900165617465973
-----------------------------
 
Loss:  22.86210209131241
-----------------------------
 
Loss:  22.82673716545105
-----------------------------
 
Loss:  22.79411256313324
-----------------------------
 
Loss:  22.765062749385834
-----------------------------
 
Loss:  22.737538814544678
-----------------------------
 
Loss:  22.712746262550354
-----------------------------
 
Loss:  22.691074013710022
-----------------------------
 
Loss:  22.67155945301056
-----------------------------
 
Loss:  22.653670608997345
-----------------------------
 
Loss:  22.636784613132477
-----------------------------
Initializing dataset...
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
6900
7000
7100
7200
7300
7400
7500
7600
7700
7800
7900
8000
8100
8200
8300
8400
8500
8600
8700
8800
8900
9000
9100
9200
9300
9400
9500
9600
9700
9800
9900
10000
10100
10200
10300
10400
10500
10600
10700
10800
10900
11000
11100
11200
11300
11400
11500
11600
11700
11800
11900
12000
12100
12200
12300
12400
12500
12600
12700
12800
12900
13000
13100
13200
13300
13400
13500
13600
13700
13800
13900
14000
14100
14200
14300
14400
14500
14600
14700
14800
14900
15000
15100
15200
15300
15400
15500
15600
15700
15800
15900
16000
16100
16200
16300
16400
16500
16600
16700
16800
16900
17000
17100
17200
17300
17400
17500
17600
17700
17800
17900
18000
18100
18200
18300
18400
18500
18600
18700
18800
18900
19000
19100
19200
19300
19400
19500
19600
19700
19800
19900
20000
20100
20200
20300
20400
20500
20600
20700
20800
20900
21000
21100
21200
21300
21400
21500
21600
21700
21800
21900
22000
22100
22200
22300
22400
22500
22600
22700
22800
22900
23000
23100
23200
23300
23400
23500
23600
23700
23800
23900
24000
24100
24200
24300
24400
24500
24600
24700
24800
24900
25000
25100
25200
25300
25400
25500
25600
25700
25800
25900
26000
26100
26200
26300
26400
26500
26600
26700
26800
26900
27000
27100
27200
27300
27400
27500
27600
27700
27800
27900
28000
28100
28200
28300
28400
28500
28600
28700
28800
28900
29000
29100
29200
29300
29400
29500
29600
29700
29800
29900
30000
30100
30200
30300
30400
30500
30600
30700
30800
30900
31000
31100
31200
31300
31400
31500
31600
31700
31800
31900
32000
32100
32200
32300
32400
32500
32600
32700
32800
32900
33000
33100
33200
33300
33400
33500
33600
33700
33800
33900
34000
34100
34200
34300
34400
34500
34600
34700
34800
34900
35000
35100
35200
35300
35400
35500
35600
35700
35800
35900
36000
36100
36200
36300
36400
36500
36600
36700
36800
36900
37000
37100
37200
37300
37400
37500
37600
37700
37800
37900
38000
38100
38200
38300
38400
38500
38600
38700
38800
38900
39000
39100
39200
39300
39400
39500
39600
39700
39800
39900
40000
40100
Dataset initialized
 
Beginning training now:
 
/home/walid_abduljalil/Normflow/loss.py:12: UserWarning: Using a target size (torch.Size([1, 1, 160, 80])) that is different to the input size (torch.Size([160, 80, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  recon_loss = torch.nn.functional.mse_loss(reconstructions, gt_images)
 
Loss:  2.73625236004591
-----------------------------
 
Loss:  2.1752607077360153
-----------------------------
 
Loss:  1.863594725728035
-----------------------------
 
Loss:  1.8086427822709084
-----------------------------
 
Loss:  1.8391720950603485
-----------------------------
 
Loss:  1.8138930201530457
-----------------------------
 
Loss:  1.7428973689675331
-----------------------------
 
Loss:  1.6635982319712639
-----------------------------
 
Loss:  1.5946926549077034
-----------------------------
 
Loss:  1.5412498265504837
-----------------------------
 
Loss:  1.501042302697897
-----------------------------
 
Loss:  1.4723931439220905
-----------------------------
 
Loss:  1.4512808062136173
-----------------------------
 
Loss:  1.434650830924511
-----------------------------
 
Loss:  1.4228707179427147
-----------------------------
 
Loss:  1.4132251031696796
-----------------------------
 
Loss:  1.4052286744117737
-----------------------------
 
Loss:  1.4001650735735893
-----------------------------
 
Loss:  1.395439263433218
-----------------------------
 
Loss:  1.3919328339397907
-----------------------------
 
Loss:  1.3886353932321072
-----------------------------
 
Loss:  1.3864918611943722
-----------------------------
 
Loss:  1.3844411820173264
-----------------------------
 
Loss:  1.3830242678523064
-----------------------------
 
Loss:  1.3818972744047642
-----------------------------
 
Loss:  1.380022056400776
-----------------------------
 
Loss:  1.3793221674859524
-----------------------------
 
Loss:  1.37935820966959
-----------------------------
 
Loss:  1.3787679374217987
-----------------------------
 
Loss:  1.3783285394310951
-----------------------------
 
Loss:  1.3775109313428402
-----------------------------
 
Loss:  1.3774769380688667
-----------------------------
 
Loss:  1.3771270401775837
-----------------------------
 
Loss:  1.3772808946669102
-----------------------------
 
Loss:  1.3765119947493076
-----------------------------
 
Loss:  1.3765978626906872
-----------------------------
 
Loss:  1.3765934854745865
-----------------------------
 
Loss:  1.376531459391117
-----------------------------
 
Loss:  1.3766339980065823
-----------------------------
 
Loss:  1.3761628419160843
-----------------------------
 
Loss:  1.3762672431766987
-----------------------------
 
Loss:  1.3762195594608784
-----------------------------
 
Loss:  1.3767757453024387
-----------------------------
 
Loss:  1.3760514557361603
-----------------------------
 
Loss:  1.3764840550720692
-----------------------------
 
Loss:  1.3766645453870296
-----------------------------
 
Loss:  1.3761512003839016
-----------------------------
 
Loss:  1.3764829374849796
-----------------------------
 
Loss:  1.3767188414931297
-----------------------------
 
Loss:  1.3766583055257797
-----------------------------
Initializing dataset...
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
6900
7000
7100
7200
7300
7400
7500
7600
7700
7800
7900
8000
8100
8200
8300
8400
8500
8600
8700
8800
8900
9000
9100
9200
9300
9400
9500
9600
9700
9800
9900
10000
10100
10200
10300
10400
10500
10600
10700
10800
10900
11000
11100
11200
11300
11400
11500
11600
11700
11800
11900
12000
12100
12200
12300
12400
12500
12600
12700
12800
12900
13000
13100
13200
13300
13400
13500
13600
13700
13800
13900
14000
14100
14200
14300
14400
14500
14600
14700
14800
14900
15000
15100
15200
15300
15400
15500
15600
15700
15800
15900
16000
16100
16200
16300
16400
16500
16600
16700
16800
16900
17000
17100
17200
17300
17400
17500
17600
17700
17800
17900
18000
18100
18200
18300
18400
18500
18600
18700
18800
18900
19000
19100
19200
19300
19400
19500
19600
19700
19800
19900
20000
20100
20200
20300
20400
20500
20600
20700
20800
20900
21000
21100
21200
21300
21400
21500
21600
21700
21800
21900
22000
22100
22200
22300
22400
22500
22600
22700
22800
22900
23000
23100
23200
23300
23400
23500
23600
23700
23800
23900
24000
24100
24200
24300
24400
24500
24600
24700
24800
24900
25000
25100
25200
25300
25400
25500
25600
25700
25800
25900
26000
26100
26200
26300
26400
26500
26600
26700
26800
26900
27000
27100
27200
27300
27400
27500
27600
27700
27800
27900
28000
28100
28200
28300
28400
28500
28600
28700
28800
28900
29000
29100
29200
29300
29400
29500
29600
29700
29800
29900
30000
30100
30200
30300
30400
30500
30600
30700
30800
30900
31000
31100
31200
31300
31400
31500
31600
31700
31800
31900
32000
32100
32200
32300
32400
32500
32600
32700
32800
32900
33000
33100
33200
33300
33400
33500
33600
33700
33800
33900
34000
34100
34200
34300
34400
34500
34600
34700
34800
34900
35000
35100
35200
35300
35400
35500
35600
35700
35800
35900
36000
36100
36200
36300
36400
36500
36600
36700
36800
36900
37000
37100
37200
37300
37400
37500
37600
37700
37800
37900
38000
38100
38200
38300
38400
38500
38600
38700
38800
38900
39000
39100
39200
39300
39400
39500
39600
39700
39800
39900
40000
40100
Dataset initialized
 
Beginning training now:
 
/home/walid_abduljalil/Normflow/loss.py:12: UserWarning: Using a target size (torch.Size([1, 1, 160, 80])) that is different to the input size (torch.Size([160, 80, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  recon_loss = torch.nn.functional.mse_loss(reconstructions, gt_images)
 
Loss:  2.5693047791719437
-----------------------------
 
Loss:  1.7704565078020096
-----------------------------
 
Loss:  1.5608489513397217
-----------------------------
 
Loss:  1.6002779826521873
-----------------------------
 
Loss:  1.6795435920357704
-----------------------------
 
Loss:  1.608683355152607
-----------------------------
 
Loss:  1.4678649604320526
-----------------------------
 
Loss:  1.3414720073342323
-----------------------------
 
Loss:  1.2654145248234272
-----------------------------
 
Loss:  1.237728912383318
-----------------------------
 
Loss:  1.2317270040512085
-----------------------------
 
Loss:  1.2257694266736507
-----------------------------
 
Loss:  1.2119099497795105
-----------------------------
 
Loss:  1.1911720037460327
-----------------------------
 
Loss:  1.166681945323944
-----------------------------
 
Loss:  1.1450275778770447
-----------------------------
 
Loss:  1.1259650811553001
-----------------------------
 
Loss:  1.1109084822237492
-----------------------------
 
Loss:  1.0989712551236153
-----------------------------
 
Loss:  1.0886534117162228
-----------------------------
 
Loss:  1.0811306536197662
-----------------------------
 
Loss:  1.0756360366940498
-----------------------------
 
Loss:  1.0713662020862103
-----------------------------
 
Loss:  1.0670439340174198
-----------------------------
 
Loss:  1.0641361586749554
-----------------------------
 
Loss:  1.0606070049107075
-----------------------------
 
Loss:  1.0584916919469833
-----------------------------
 
Loss:  1.0553432628512383
-----------------------------
 
Loss:  1.0531754232943058
-----------------------------
 
Loss:  1.0511156171560287
-----------------------------
 
Loss:  1.049189455807209
-----------------------------
 
Loss:  1.0469483211636543
-----------------------------
 
Loss:  1.0451173409819603
-----------------------------
 
Loss:  1.0430551134049892
-----------------------------
 
Loss:  1.0423758998513222
-----------------------------
 
Loss:  1.0405355133116245
-----------------------------
 
Loss:  1.0401496663689613
-----------------------------
 
Loss:  1.0381498374044895
-----------------------------
 
Loss:  1.0372788645327091
-----------------------------
 
Loss:  1.0370162315666676
-----------------------------
 
Loss:  1.0355792008340359
-----------------------------
 
Loss:  1.0351131670176983
-----------------------------
 
Loss:  1.034722849726677
-----------------------------
 
Loss:  1.0340151377022266
-----------------------------
 
Loss:  1.0342621244490147
-----------------------------
 
Loss:  1.0333608835935593
-----------------------------
 
Loss:  1.0336226783692837
-----------------------------
 
Loss:  1.032685674726963
-----------------------------
 
Loss:  1.0318867862224579
-----------------------------
 
Loss:  1.03202685713768
-----------------------------
Initializing dataset...
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
6900
7000
7100
7200
7300
7400
7500
7600
7700
7800
7900
8000
8100
8200
8300
8400
8500
8600
8700
8800
8900
9000
9100
9200
9300
9400
9500
9600
9700
9800
9900
10000
10100
10200
10300
10400
10500
10600
10700
10800
10900
11000
11100
11200
11300
11400
11500
11600
11700
11800
11900
12000
12100
12200
12300
12400
12500
12600
12700
12800
12900
13000
13100
13200
13300
13400
13500
13600
13700
13800
13900
14000
14100
14200
14300
14400
14500
14600
14700
14800
14900
15000
15100
15200
15300
15400
15500
15600
15700
15800
15900
16000
16100
16200
16300
16400
16500
16600
16700
16800
16900
17000
17100
17200
17300
17400
17500
17600
17700
17800
17900
18000
18100
18200
18300
18400
18500
18600
18700
18800
18900
19000
19100
19200
19300
19400
19500
19600
19700
19800
19900
20000
20100
20200
20300
20400
20500
20600
20700
20800
20900
21000
21100
21200
21300
21400
21500
21600
21700
21800
21900
22000
22100
22200
22300
22400
22500
22600
22700
22800
22900
23000
23100
23200
23300
23400
23500
23600
23700
23800
23900
24000
24100
24200
24300
24400
24500
24600
24700
24800
24900
25000
25100
25200
25300
25400
25500
25600
25700
25800
25900
26000
26100
26200
26300
26400
26500
26600
26700
26800
26900
27000
27100
27200
27300
27400
27500
27600
27700
27800
27900
28000
28100
28200
28300
28400
28500
28600
28700
28800
28900
29000
29100
29200
29300
29400
29500
29600
29700
29800
29900
30000
30100
30200
30300
30400
30500
30600
30700
30800
30900
31000
31100
31200
31300
31400
31500
31600
31700
31800
31900
32000
32100
32200
32300
32400
32500
32600
32700
32800
32900
33000
33100
33200
33300
33400
33500
33600
33700
33800
33900
34000
34100
34200
34300
34400
34500
34600
34700
34800
34900
35000
35100
35200
35300
35400
35500
35600
35700
35800
35900
36000
36100
36200
36300
36400
36500
36600
36700
36800
36900
37000
37100
37200
37300
37400
37500
37600
37700
37800
37900
38000
38100
38200
38300
38400
38500
38600
38700
38800
38900
39000
39100
39200
39300
39400
39500
39600
39700
39800
39900
40000
40100
Dataset initialized
 
Beginning training now:
 
/home/walid_abduljalil/Normflow/loss.py:12: UserWarning: Using a target size (torch.Size([1, 1, 160, 80])) that is different to the input size (torch.Size([160, 80, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  recon_loss = torch.nn.functional.mse_loss(reconstructions, gt_images)
 
Loss:  2.8598761186003685
-----------------------------
 
Loss:  1.8486080691218376
-----------------------------
 
Loss:  1.6722308471798897
-----------------------------
 
Loss:  1.745656132698059
-----------------------------
 
Loss:  1.7905864864587784
-----------------------------
 
Loss:  1.7105333507061005
-----------------------------
 
Loss:  1.5856694430112839
-----------------------------
 
Loss:  1.4761975966393948
-----------------------------
 
Loss:  1.407347060739994
-----------------------------
 
Loss:  1.3718433678150177
-----------------------------
 
Loss:  1.3492759317159653
-----------------------------
 
Loss:  1.3276583515107632
-----------------------------
 
Loss:  1.298846211284399
-----------------------------
 
Loss:  1.2675544247031212
-----------------------------
 
Loss:  1.236187107861042
-----------------------------
 
Loss:  1.2072478421032429
-----------------------------
 
Loss:  1.1827793903648853
-----------------------------
 
Loss:  1.161631289869547
-----------------------------
 
Loss:  1.1449459940195084
-----------------------------
 
Loss:  1.1317004449665546
-----------------------------
 
Loss:  1.12078208476305
-----------------------------
 
Loss:  1.1117024347186089
-----------------------------
 
Loss:  1.1039773933589458
-----------------------------
 
Loss:  1.0971331968903542
-----------------------------
 
Loss:  1.092403568327427
-----------------------------
 
Loss:  1.0878401808440685
-----------------------------
 
Loss:  1.0844727046787739
-----------------------------
 
Loss:  1.0804053395986557
-----------------------------
 
Loss:  1.0775262489914894
-----------------------------
 
Loss:  1.0750608518719673
-----------------------------
 
Loss:  1.0724549181759357
-----------------------------
 
Loss:  1.070754136890173
-----------------------------
 
Loss:  1.0689698159694672
-----------------------------
 
Loss:  1.0671289637684822
-----------------------------
 
Loss:  1.0658139362931252
-----------------------------
 
Loss:  1.0647643357515335
-----------------------------
 
Loss:  1.063756924122572
-----------------------------
 
Loss:  1.0626869276165962
-----------------------------
 
Loss:  1.0619948618113995
-----------------------------
 
Loss:  1.0613244026899338
-----------------------------
 
Loss:  1.0610522702336311
-----------------------------
 
Loss:  1.0601305402815342
-----------------------------
 
Loss:  1.0594558902084827
-----------------------------
 
Loss:  1.0592898353934288
-----------------------------
 
Loss:  1.0587288066744804
-----------------------------
 
Loss:  1.057844702154398
-----------------------------
 
Loss:  1.0586588643491268
-----------------------------
 
Loss:  1.05773052200675
-----------------------------
 
Loss:  1.0578741319477558
-----------------------------
 
Loss:  1.0575874708592892
-----------------------------
 
Loss:  1.0575510561466217
-----------------------------
 
Loss:  1.0575015097856522
-----------------------------
 
Loss:  1.0568148456513882
-----------------------------
 
Loss:  1.0568520985543728
-----------------------------
 
Loss:  1.0565011762082577
-----------------------------
 
Loss:  1.056873518973589
-----------------------------
 
Loss:  1.0559678077697754
-----------------------------
 
Loss:  1.0563789866864681
-----------------------------
 
Loss:  1.056567020714283
-----------------------------
 
Loss:  1.0560312308371067
-----------------------------
 
Loss:  1.056908257305622
-----------------------------
 
Loss:  1.0562799870967865
-----------------------------
 
Loss:  1.0565387085080147
-----------------------------
 
Loss:  1.0559221729636192
-----------------------------
 
Loss:  1.056486088782549
-----------------------------
 
Loss:  1.056066807359457
-----------------------------
 
Loss:  1.0560772381722927
-----------------------------
 
Loss:  1.055891439318657
-----------------------------
 
Loss:  1.0563410818576813
-----------------------------
 
Loss:  1.0561555624008179
-----------------------------
 
Loss:  1.0560614056885242
-----------------------------
 
Loss:  1.0559475049376488
-----------------------------
 
Loss:  1.0555824264883995
-----------------------------
 
Loss:  1.05583555996418
-----------------------------
 
Loss:  1.0561447590589523
-----------------------------
 
Loss:  1.0552318766713142
-----------------------------
 
Loss:  1.0559147223830223
-----------------------------
 
Loss:  1.0561994276940823
-----------------------------
 
Loss:  1.0559884831309319
-----------------------------
 
Loss:  1.055828481912613
-----------------------------
 
Loss:  1.0560369119048119
-----------------------------
 
Loss:  1.055798213928938
-----------------------------
 
Loss:  1.0558961890637875
-----------------------------
 
Loss:  1.055818609893322
-----------------------------
 
Loss:  1.0558631271123886
-----------------------------
 
Loss:  1.0557785630226135
-----------------------------
 
Loss:  1.0556944645941257
-----------------------------
 
Loss:  1.055898517370224
-----------------------------
 
Loss:  1.0559839196503162
-----------------------------
 
Loss:  1.0558546520769596
-----------------------------
 
Loss:  1.0564927011728287
-----------------------------
 
Loss:  1.0561314411461353
-----------------------------
 
Loss:  1.0560480877757072
-----------------------------
 
Loss:  1.0556811466813087
-----------------------------
 
Loss:  1.055841613560915
-----------------------------
 
Loss:  1.0559148155152798
-----------------------------
 
Loss:  1.0561785660684109
-----------------------------
 
Loss:  1.056008692830801
-----------------------------
 
Loss:  1.0555361397564411
-----------------------------
 
Loss:  1.0552181862294674
-----------------------------
Initializing dataset...
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
6900
7000
7100
7200
7300
7400
7500
7600
7700
7800
7900
8000
8100
8200
8300
8400
8500
8600
8700
8800
8900
9000
9100
9200
9300
9400
9500
9600
9700
9800
9900
10000
10100
10200
10300
10400
10500
10600
10700
10800
10900
11000
11100
11200
11300
11400
11500
11600
11700
11800
11900
12000
12100
12200
12300
12400
12500
12600
12700
12800
12900
13000
13100
13200
13300
13400
13500
13600
13700
13800
13900
14000
14100
14200
14300
14400
14500
14600
14700
14800
14900
15000
15100
15200
15300
15400
15500
15600
15700
15800
15900
16000
16100
16200
16300
16400
16500
16600
16700
16800
16900
17000
17100
17200
17300
17400
17500
17600
17700
17800
17900
18000
18100
18200
18300
18400
18500
18600
18700
18800
18900
19000
19100
19200
19300
19400
19500
19600
19700
19800
19900
20000
20100
20200
20300
20400
20500
20600
20700
20800
20900
21000
21100
21200
21300
21400
21500
21600
21700
21800
21900
22000
22100
22200
22300
22400
22500
22600
22700
22800
22900
23000
23100
23200
23300
23400
23500
23600
23700
23800
23900
24000
24100
24200
24300
24400
24500
24600
24700
24800
24900
25000
25100
25200
25300
25400
25500
25600
25700
25800
25900
26000
26100
26200
26300
26400
26500
26600
26700
26800
26900
27000
27100
27200
27300
27400
27500
27600
27700
27800
27900
28000
28100
28200
28300
28400
28500
28600
28700
28800
28900
29000
29100
29200
29300
29400
29500
29600
29700
29800
29900
30000
30100
30200
30300
30400
30500
30600
30700
30800
30900
31000
31100
31200
31300
31400
31500
31600
31700
31800
31900
32000
32100
32200
32300
32400
32500
32600
32700
32800
32900
33000
33100
33200
33300
33400
33500
33600
33700
33800
33900
34000
34100
34200
34300
34400
34500
34600
34700
34800
34900
35000
35100
35200
35300
35400
35500
35600
35700
35800
35900
36000
36100
36200
36300
36400
36500
36600
36700
36800
36900
37000
37100
37200
37300
37400
37500
37600
37700
37800
37900
38000
38100
38200
38300
38400
38500
38600
38700
38800
38900
39000
39100
39200
39300
39400
39500
39600
39700
39800
39900
40000
40100
Dataset initialized
 
Beginning training now:
 
/home/walid_abduljalil/Normflow/loss.py:12: UserWarning: Using a target size (torch.Size([1, 1, 160, 80])) that is different to the input size (torch.Size([160, 80, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  recon_loss = torch.nn.functional.mse_loss(reconstructions, gt_images)
 
Loss:  2.527742460370064
-----------------------------
 
Loss:  2.13258545845747
-----------------------------
 
Loss:  1.79247185587883
-----------------------------
 
Loss:  1.8277322873473167
-----------------------------
 
Loss:  1.8937977030873299
-----------------------------
 
Loss:  1.8250742927193642
-----------------------------
 
Loss:  1.6913069412112236
-----------------------------
 
Loss:  1.5555762685835361
-----------------------------
 
Loss:  1.4518633484840393
-----------------------------
 
Loss:  1.389456633478403
-----------------------------
 
Loss:  1.3559057377278805
-----------------------------
 
Loss:  1.3366847299039364
-----------------------------
 
Loss:  1.3215146958827972
-----------------------------
 
Loss:  1.3063727878034115
-----------------------------
 
Loss:  1.2894718907773495
-----------------------------
 
Loss:  1.271868497133255
-----------------------------
 
Loss:  1.2562554329633713
-----------------------------
 
Loss:  1.2425027787685394
-----------------------------
 
Loss:  1.2312560342252254
-----------------------------
 
Loss:  1.2221037410199642
-----------------------------
 
Loss:  1.21584702283144
-----------------------------
 
Loss:  1.2111470103263855
-----------------------------
 
Loss:  1.207298319786787
-----------------------------
 
Loss:  1.2048021890223026
-----------------------------
 
Loss:  1.2024415656924248
-----------------------------
 
Loss:  1.2000747956335545
-----------------------------
 
Loss:  1.1987355537712574
-----------------------------
 
Loss:  1.1970486491918564
-----------------------------
 
Loss:  1.1958646588027477
-----------------------------
 
Loss:  1.1938072741031647
-----------------------------
 
Loss:  1.1925119906663895
-----------------------------
 
Loss:  1.1913042515516281
-----------------------------
 
Loss:  1.1902122758328915
-----------------------------
 
Loss:  1.1895638890564442
-----------------------------
 
Loss:  1.1887433007359505
-----------------------------
 
Loss:  1.187689881771803
-----------------------------
 
Loss:  1.1867627501487732
-----------------------------
 
Loss:  1.1864244937896729
-----------------------------
 
Loss:  1.1861340142786503
-----------------------------
 
Loss:  1.1855452321469784
-----------------------------
 
Loss:  1.184696052223444
-----------------------------
 
Loss:  1.1846214532852173
-----------------------------
 
Loss:  1.1842133477330208
-----------------------------
 
Loss:  1.183590106666088
-----------------------------
 
Loss:  1.1831889860332012
-----------------------------
 
Loss:  1.183339487761259
-----------------------------
 
Loss:  1.1833610944449902
-----------------------------
 
Loss:  1.1832674965262413
-----------------------------
 
Loss:  1.1826859787106514
-----------------------------
 
Loss:  1.1829867959022522
-----------------------------
 
Loss:  1.1824681423604488
-----------------------------
 
Loss:  1.1822022497653961
-----------------------------
 
Loss:  1.181686483323574
-----------------------------
 
Loss:  1.1819671839475632
-----------------------------
 
Loss:  1.181680802255869
-----------------------------
 
Loss:  1.1814495548605919
-----------------------------
 
Loss:  1.1818389408290386
-----------------------------
 
Loss:  1.1815820820629597
-----------------------------
 
Loss:  1.1813316494226456
-----------------------------
 
Loss:  1.182006299495697
-----------------------------
 
Loss:  1.1815142817795277
-----------------------------
 
Loss:  1.181396096944809
-----------------------------
 
Loss:  1.1813897639513016
-----------------------------
 
Loss:  1.181168295443058
-----------------------------
 
Loss:  1.1806868016719818
-----------------------------
 
Loss:  1.181763969361782
-----------------------------
 
Loss:  1.1815904639661312
-----------------------------
 
Loss:  1.1812048964202404
-----------------------------
 
Loss:  1.181267388164997
-----------------------------
 
Loss:  1.1815526522696018
-----------------------------
 
Loss:  1.1813797056674957
-----------------------------
 
Loss:  1.1816713958978653
-----------------------------
 
Loss:  1.1811597272753716
-----------------------------
 
Loss:  1.1816030368208885
-----------------------------
 
Loss:  1.1810068972408772
-----------------------------
 
Loss:  1.180964708328247
-----------------------------
 
Loss:  1.1813418939709663
-----------------------------
 
Loss:  1.1813324876129627
-----------------------------
 
Loss:  1.181781105697155
-----------------------------
 
Loss:  1.1815095320343971
-----------------------------
 
Loss:  1.1812983080744743
-----------------------------
 
Loss:  1.1813722550868988
-----------------------------
 
Loss:  1.1813132092356682
-----------------------------
 
Loss:  1.181400939822197
-----------------------------
 
Loss:  1.1813540942966938
-----------------------------
 
Loss:  1.1810820549726486
-----------------------------
 
Loss:  1.1811580508947372
-----------------------------
 
Loss:  1.18137551471591
-----------------------------
 
Loss:  1.1817284859716892
-----------------------------
 
Loss:  1.181466318666935
-----------------------------
 
Loss:  1.1811601929366589
-----------------------------
 
Loss:  1.1814660392701626
-----------------------------
 
Loss:  1.1813625693321228
-----------------------------
 
Loss:  1.1815134435892105
-----------------------------
 
Loss:  1.1814307421445847
-----------------------------
 
Loss:  1.1812149547040462
-----------------------------
 
Loss:  1.1812142096459866
-----------------------------
 
Loss:  1.1814983561635017
-----------------------------
 
Loss:  1.1813427321612835
-----------------------------
 
Loss:  1.1811710894107819
-----------------------------
Initializing dataset...
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
6900
7000
7100
7200
7300
7400
7500
7600
7700
7800
7900
8000
8100
8200
8300
8400
8500
8600
8700
8800
8900
9000
9100
9200
9300
9400
9500
9600
9700
9800
9900
10000
10100
10200
10300
10400
10500
10600
10700
10800
10900
11000
11100
11200
11300
11400
11500
11600
11700
11800
11900
12000
12100
12200
12300
12400
12500
12600
12700
12800
12900
13000
13100
13200
13300
13400
13500
13600
13700
13800
13900
14000
14100
14200
14300
14400
14500
14600
14700
14800
14900
15000
15100
15200
15300
15400
15500
15600
15700
15800
15900
16000
16100
16200
16300
16400
16500
16600
16700
16800
16900
17000
17100
17200
17300
17400
17500
17600
17700
17800
17900
18000
18100
18200
18300
18400
18500
18600
18700
18800
18900
19000
19100
19200
19300
19400
19500
19600
19700
19800
19900
20000
20100
20200
20300
20400
20500
20600
20700
20800
20900
21000
21100
21200
21300
21400
21500
21600
21700
21800
21900
22000
22100
22200
22300
22400
22500
22600
22700
22800
22900
23000
23100
23200
23300
23400
23500
23600
23700
23800
23900
24000
24100
24200
24300
24400
24500
24600
24700
24800
24900
25000
25100
25200
25300
25400
25500
25600
25700
25800
25900
26000
26100
26200
26300
26400
26500
26600
26700
26800
26900
27000
27100
27200
27300
27400
27500
27600
27700
27800
27900
28000
28100
28200
28300
28400
28500
28600
28700
28800
28900
29000
29100
29200
29300
29400
29500
29600
29700
29800
29900
30000
30100
30200
30300
30400
30500
30600
30700
30800
30900
31000
31100
31200
31300
31400
31500
31600
31700
31800
31900
32000
32100
32200
32300
32400
32500
32600
32700
32800
32900
33000
33100
33200
33300
33400
33500
33600
33700
33800
33900
34000
34100
34200
34300
34400
34500
34600
34700
34800
34900
35000
35100
35200
35300
35400
35500
35600
35700
35800
35900
36000
36100
36200
36300
36400
36500
36600
36700
36800
36900
37000
37100
37200
37300
37400
37500
37600
37700
37800
37900
38000
38100
38200
38300
38400
38500
38600
38700
38800
38900
39000
39100
39200
39300
39400
39500
39600
39700
39800
39900
40000
40100
Dataset initialized
 
Beginning training now:
 
/home/walid_abduljalil/Normflow/loss.py:12: UserWarning: Using a target size (torch.Size([1, 1, 160, 80])) that is different to the input size (torch.Size([160, 80, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  recon_loss = torch.nn.functional.mse_loss(reconstructions, gt_images)
 
Loss:  2.207021787762642
-----------------------------
 
Loss:  202.04238891601562
-----------------------------
 
Loss:  6.397227942943573
-----------------------------
 
Loss:  7.303979247808456
-----------------------------
 
Loss:  12.595751881599426
-----------------------------
 
Loss:  12.298395484685898
-----------------------------
 
Loss:  10.656071454286575
-----------------------------
 
Loss:  10.030589252710342
-----------------------------
 
Loss:  10.52936464548111
-----------------------------
 
Loss:  11.212407052516937
-----------------------------
 
Loss:  11.520678550004959
-----------------------------
 
Loss:  11.263954639434814
-----------------------------
 
Loss:  10.590602457523346
-----------------------------
 
Loss:  9.722142666578293
-----------------------------
 
Loss:  8.886631578207016
-----------------------------
 
Loss:  8.212101459503174
-----------------------------
 
Loss:  7.703917473554611
-----------------------------
 
Loss:  7.314182817935944
-----------------------------
 
Loss:  6.99218362569809
-----------------------------
 
Loss:  6.6949352622032166
-----------------------------
 
Loss:  6.3977643847465515
-----------------------------
 
Loss:  6.087743490934372
-----------------------------
 
Loss:  5.765555426478386
-----------------------------
 
Loss:  5.438636243343353
-----------------------------
 
Loss:  5.12029267847538
-----------------------------
 
Loss:  4.820967465639114
-----------------------------
 
Loss:  4.5459069311618805
-----------------------------
 
Loss:  4.297101125121117
-----------------------------
 
Loss:  4.075820744037628
-----------------------------
 
Loss:  3.8786448538303375
-----------------------------
 
Loss:  3.7044715136289597
-----------------------------
 
Loss:  3.549373894929886
-----------------------------
 
Loss:  3.412572667002678
-----------------------------
 
Loss:  3.289373219013214
-----------------------------
 
Loss:  3.1798627227544785
-----------------------------
 
Loss:  3.082067519426346
-----------------------------
 
Loss:  2.994418330490589
-----------------------------
 
Loss:  2.915724366903305
-----------------------------
 
Loss:  2.844211272895336
-----------------------------
 
Loss:  2.780812606215477
-----------------------------
 
Loss:  2.7232717722654343
-----------------------------
 
Loss:  2.6720119640231133
-----------------------------
 
Loss:  2.625872753560543
-----------------------------
 
Loss:  2.5837983936071396
-----------------------------
 
Loss:  2.546258829534054
-----------------------------
 
Loss:  2.5129808112978935
-----------------------------
 
Loss:  2.48248353600502
-----------------------------
 
Loss:  2.4554161354899406
-----------------------------
 
Loss:  2.4309853091835976
-----------------------------
 
Loss:  2.409554086625576
-----------------------------
 
Loss:  2.389499731361866
-----------------------------
 
Loss:  2.3718025535345078
-----------------------------
 
Loss:  2.3552393540740013
-----------------------------
 
Loss:  2.341259643435478
-----------------------------
 
Loss:  2.329007536172867
-----------------------------
 
Loss:  2.3173781111836433
-----------------------------
 
Loss:  2.3066699504852295
-----------------------------
 
Loss:  2.2979026660323143
-----------------------------
 
Loss:  2.289208024740219
-----------------------------
 
Loss:  2.281849645078182
-----------------------------
 
Loss:  2.274162322282791
-----------------------------
 
Loss:  2.268902212381363
-----------------------------
 
Loss:  2.263028733432293
-----------------------------
 
Loss:  2.2593047469854355
-----------------------------
 
Loss:  2.2535383701324463
-----------------------------
 
Loss:  2.2503001615405083
-----------------------------
 
Loss:  2.2459203377366066
-----------------------------
 
Loss:  2.2433096542954445
-----------------------------
 
Loss:  2.240007184445858
-----------------------------
 
Loss:  2.237272821366787
-----------------------------
 
Loss:  2.2349318489432335
-----------------------------
 
Loss:  2.2330937907099724
-----------------------------
 
Loss:  2.2307218983769417
-----------------------------
 
Loss:  2.229212038218975
-----------------------------
 
Loss:  2.2280508652329445
-----------------------------
 
Loss:  2.2261882200837135
-----------------------------
 
Loss:  2.2244971245527267
-----------------------------
 
Loss:  2.2240981459617615
-----------------------------
 
Loss:  2.222830057144165
-----------------------------
 
Loss:  2.222076617181301
-----------------------------
 
Loss:  2.2212769836187363
-----------------------------
 
Loss:  2.220342308282852
-----------------------------
 
Loss:  2.2196922451257706
-----------------------------
 
Loss:  2.2187912836670876
-----------------------------
 
Loss:  2.2186269983649254
-----------------------------
 
Loss:  2.218109928071499
-----------------------------
 
Loss:  2.2171514108777046
-----------------------------
 
Loss:  2.2176098078489304
-----------------------------
 
Loss:  2.2167446091771126
-----------------------------
 
Loss:  2.216225676238537
-----------------------------
 
Loss:  2.216108702123165
-----------------------------
 
Loss:  2.2161925211548805
-----------------------------
 
Loss:  2.215561643242836
-----------------------------
 
Loss:  2.2153593599796295
-----------------------------
 
Loss:  2.215053327381611
-----------------------------
 
Loss:  2.215082198381424
-----------------------------
 
Loss:  2.2151105105876923
-----------------------------
 
Loss:  2.215038426220417
-----------------------------
 
Loss:  2.214724011719227
-----------------------------
 
Loss:  2.2144123911857605
-----------------------------
Initializing dataset...
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
6900
7000
7100
7200
7300
7400
7500
7600
7700
7800
7900
8000
8100
8200
8300
8400
8500
8600
8700
8800
8900
9000
9100
9200
9300
9400
9500
9600
9700
9800
9900
10000
10100
10200
10300
10400
10500
10600
10700
10800
10900
11000
11100
11200
11300
11400
11500
11600
11700
11800
11900
12000
12100
12200
12300
12400
12500
12600
12700
12800
12900
13000
13100
13200
13300
13400
13500
13600
13700
13800
13900
14000
14100
14200
14300
14400
14500
14600
14700
14800
14900
15000
15100
15200
15300
15400
15500
15600
15700
15800
15900
16000
16100
16200
16300
16400
16500
16600
16700
16800
16900
17000
17100
17200
17300
17400
17500
17600
17700
17800
17900
18000
18100
18200
18300
18400
18500
18600
18700
18800
18900
19000
19100
19200
19300
19400
19500
19600
19700
19800
19900
20000
20100
20200
20300
20400
20500
20600
20700
20800
20900
21000
21100
21200
21300
21400
21500
21600
21700
21800
21900
22000
22100
22200
22300
22400
22500
22600
22700
22800
22900
23000
23100
23200
23300
23400
23500
23600
23700
23800
23900
24000
24100
24200
24300
24400
24500
24600
24700
24800
24900
25000
25100
25200
25300
25400
25500
25600
25700
25800
25900
26000
26100
26200
26300
26400
26500
26600
26700
26800
26900
27000
27100
27200
27300
27400
27500
27600
27700
27800
27900
28000
28100
28200
28300
28400
28500
28600
28700
28800
28900
29000
29100
29200
29300
29400
29500
29600
29700
29800
29900
30000
30100
30200
30300
30400
30500
30600
30700
30800
30900
31000
31100
31200
31300
31400
31500
31600
31700
31800
31900
32000
32100
32200
32300
32400
32500
32600
32700
32800
32900
33000
33100
33200
33300
33400
33500
33600
33700
33800
33900
34000
34100
34200
34300
34400
34500
34600
34700
34800
34900
35000
35100
35200
35300
35400
35500
35600
35700
35800
35900
36000
36100
36200
36300
36400
36500
36600
36700
36800
36900
37000
37100
37200
37300
37400
37500
37600
37700
37800
37900
38000
38100
38200
38300
38400
38500
38600
38700
38800
38900
39000
39100
39200
39300
39400
39500
39600
39700
39800
39900
40000
40100
Dataset initialized
 
Beginning training now:
 
/home/walid_abduljalil/Normflow/loss.py:12: UserWarning: Using a target size (torch.Size([1, 1, 160, 80])) that is different to the input size (torch.Size([160, 80, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  recon_loss = torch.nn.functional.mse_loss(reconstructions, gt_images)
 
Loss:  2.7481744065880775
-----------------------------
 
Loss:  1.9724365323781967
-----------------------------
 
Loss:  1.7923949286341667
-----------------------------
 
Loss:  1.8501833081245422
-----------------------------
 
Loss:  1.8960034474730492
-----------------------------
 
Loss:  1.8046209588646889
-----------------------------
 
Loss:  1.6629943624138832
-----------------------------
 
Loss:  1.5438924543559551
-----------------------------
 
Loss:  1.469269022345543
-----------------------------
 
Loss:  1.4305660501122475
-----------------------------
 
Loss:  1.409111451357603
-----------------------------
 
Loss:  1.3901455327868462
-----------------------------
 
Loss:  1.3687459751963615
-----------------------------
 
Loss:  1.3447240926325321
-----------------------------
 
Loss:  1.3222233392298222
-----------------------------
 
Loss:  1.3024763204157352
-----------------------------
 
Loss:  1.2858185917139053
-----------------------------
 
Loss:  1.2724176980555058
-----------------------------
 
Loss:  1.2608570978045464
-----------------------------
 
Loss:  1.2511922046542168
-----------------------------
 
Loss:  1.2424184940755367
-----------------------------
 
Loss:  1.2352410703897476
-----------------------------
 
Loss:  1.2285242788493633
-----------------------------
 
Loss:  1.2229588814079762
-----------------------------
 
Loss:  1.217553485184908
-----------------------------
 
Loss:  1.2132939882576466
-----------------------------
 
Loss:  1.209143828600645
-----------------------------
 
Loss:  1.2052413076162338
-----------------------------
 
Loss:  1.202471274882555
-----------------------------
 
Loss:  1.1999012902379036
-----------------------------
 
Loss:  1.1974002234637737
-----------------------------
 
Loss:  1.195456925779581
-----------------------------
 
Loss:  1.1937720701098442
-----------------------------
 
Loss:  1.1923519894480705
-----------------------------
 
Loss:  1.1907988227903843
-----------------------------
 
Loss:  1.1897838674485683
-----------------------------
 
Loss:  1.1887602508068085
-----------------------------
 
Loss:  1.187784317880869
-----------------------------
 
Loss:  1.1873592622578144
-----------------------------
 
Loss:  1.1862455867230892
-----------------------------
 
Loss:  1.1858826503157616
-----------------------------
 
Loss:  1.1853323318064213
-----------------------------
 
Loss:  1.1846080422401428
-----------------------------
 
Loss:  1.1844217777252197
-----------------------------
 
Loss:  1.1840932071208954
-----------------------------
 
Loss:  1.183873601257801
-----------------------------
 
Loss:  1.183471456170082
-----------------------------
 
Loss:  1.1832955293357372
-----------------------------
 
Loss:  1.1825760826468468
-----------------------------
 
Loss:  1.1827483773231506
-----------------------------
 
Loss:  1.1829088442027569
-----------------------------
 
Loss:  1.1822992004454136
-----------------------------
 
Loss:  1.182375755161047
-----------------------------
 
Loss:  1.182285137474537
-----------------------------
 
Loss:  1.1818133294582367
-----------------------------
 
Loss:  1.181949395686388
-----------------------------
 
Loss:  1.1817142367362976
-----------------------------
 
Loss:  1.1816086247563362
-----------------------------
 
Loss:  1.182285975664854
-----------------------------
 
Loss:  1.1819897219538689
-----------------------------
 
Loss:  1.1815902777016163
-----------------------------
 
Loss:  1.1817139573395252
-----------------------------
 
Loss:  1.1814910918474197
-----------------------------
 
Loss:  1.1814780533313751
-----------------------------
 
Loss:  1.1811481788754463
-----------------------------
 
Loss:  1.1813788674771786
-----------------------------
 
Loss:  1.1815933510661125
-----------------------------
 
Loss:  1.1813822202384472
-----------------------------
 
Loss:  1.1809959076344967
-----------------------------
 
Loss:  1.1817033402621746
-----------------------------
 
Loss:  1.1812522076070309
-----------------------------
 
Loss:  1.1810875497758389
-----------------------------
 
Loss:  1.1810625903308392
-----------------------------
 
Loss:  1.1813675053417683
-----------------------------
 
Loss:  1.1813752353191376
-----------------------------
 
Loss:  1.1821244843304157
-----------------------------
 
Loss:  1.1810371652245522
-----------------------------
 
Loss:  1.1816528625786304
-----------------------------
 
Loss:  1.1817949824035168
-----------------------------
 
Loss:  1.1810996569693089
-----------------------------
 
Loss:  1.181760337203741
-----------------------------
 
Loss:  1.1813415214419365
-----------------------------
 
Loss:  1.1812904849648476
-----------------------------
 
Loss:  1.181026455014944
-----------------------------
 
Loss:  1.1811387725174427
-----------------------------
 
Loss:  1.1810528114438057
-----------------------------
 
Loss:  1.18120601400733
-----------------------------
 
Loss:  1.1812012642621994
-----------------------------
 
Loss:  1.181629579514265
-----------------------------
 
Loss:  1.1813290417194366
-----------------------------
 
Loss:  1.1809621937572956
-----------------------------
 
Loss:  1.1815933510661125
-----------------------------
 
Loss:  1.1810585856437683
-----------------------------
 
Loss:  1.181740965694189
-----------------------------
 
Loss:  1.1812730692327023
-----------------------------
 
Loss:  1.181038748472929
-----------------------------
 
Loss:  1.1807520873844624
-----------------------------
 
Loss:  1.1810771189630032
-----------------------------
 
Loss:  1.1815950274467468
-----------------------------
 
Loss:  1.1812572367489338
-----------------------------
Initializing dataset...
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
6900
7000
7100
7200
7300
7400
7500
7600
7700
7800
7900
8000
8100
8200
8300
8400
8500
8600
8700
8800
8900
9000
9100
9200
9300
9400
9500
9600
9700
9800
9900
10000
10100
10200
10300
10400
10500
10600
10700
10800
10900
11000
11100
11200
11300
11400
11500
11600
11700
11800
11900
12000
12100
12200
12300
12400
12500
12600
12700
12800
12900
13000
13100
13200
13300
13400
13500
13600
13700
13800
13900
14000
14100
14200
14300
14400
14500
14600
14700
14800
14900
15000
15100
15200
15300
15400
15500
15600
15700
15800
15900
16000
16100
16200
16300
16400
16500
16600
16700
16800
16900
17000
17100
17200
17300
17400
17500
17600
17700
17800
17900
18000
18100
18200
18300
18400
18500
18600
18700
18800
18900
19000
19100
19200
19300
19400
19500
19600
19700
19800
19900
20000
20100
20200
20300
20400
20500
20600
20700
20800
20900
21000
21100
21200
21300
21400
21500
21600
21700
21800
21900
22000
22100
22200
22300
22400
22500
22600
22700
22800
22900
23000
23100
23200
23300
23400
23500
23600
23700
23800
23900
24000
24100
24200
24300
24400
24500
24600
24700
24800
24900
25000
25100
25200
25300
25400
25500
25600
25700
25800
25900
26000
26100
26200
26300
26400
26500
26600
26700
26800
26900
27000
27100
27200
27300
27400
27500
27600
27700
27800
27900
28000
28100
28200
28300
28400
28500
28600
28700
28800
28900
29000
29100
29200
29300
29400
29500
29600
29700
29800
29900
30000
30100
30200
30300
30400
30500
30600
30700
30800
30900
31000
31100
31200
31300
31400
31500
31600
31700
31800
31900
32000
32100
32200
32300
32400
32500
32600
32700
32800
32900
33000
33100
33200
33300
33400
33500
33600
33700
33800
33900
34000
34100
34200
34300
34400
34500
34600
34700
34800
34900
35000
35100
35200
35300
35400
35500
35600
35700
35800
35900
36000
36100
36200
36300
36400
36500
36600
36700
36800
36900
37000
37100
37200
37300
37400
37500
37600
37700
37800
37900
38000
38100
38200
38300
38400
38500
38600
38700
38800
38900
39000
39100
39200
39300
39400
39500
39600
39700
39800
39900
40000
40100
Dataset initialized
 
Beginning training now:
 
/home/walid_abduljalil/Normflow/loss.py:12: UserWarning: Using a target size (torch.Size([1, 1, 160, 80])) that is different to the input size (torch.Size([160, 80, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  recon_loss = torch.nn.functional.mse_loss(reconstructions, gt_images)
 
Loss:  2.485739439725876
-----------------------------
 
Loss:  2.043982222676277
-----------------------------
 
Loss:  1.6707662492990494
-----------------------------
 
Loss:  1.840774156153202
-----------------------------
 
Loss:  1.9078267738223076
-----------------------------
 
Loss:  1.7371214926242828
-----------------------------
 
Loss:  1.5944218263030052
-----------------------------
 
Loss:  1.5595107339322567
-----------------------------
 
Loss:  1.5663370490074158
-----------------------------
 
Loss:  1.5246526338160038
-----------------------------
 
Loss:  1.4377105049788952
-----------------------------
 
Loss:  1.3625329360365868
-----------------------------
 
Loss:  1.316987443715334
-----------------------------
 
Loss:  1.2850494123995304
-----------------------------
 
Loss:  1.2629908509552479
-----------------------------
 
Loss:  1.2433402240276337
-----------------------------
 
Loss:  1.2150568887591362
-----------------------------
 
Loss:  1.1877292767167091
-----------------------------
 
Loss:  1.1775380931794643
-----------------------------
 
Loss:  1.1793511919677258
-----------------------------
 
Loss:  1.1703836731612682
-----------------------------
 
Loss:  1.1479656212031841
-----------------------------
 
Loss:  1.1242552660405636
-----------------------------
 
Loss:  1.106267049908638
-----------------------------
 
Loss:  1.0948766022920609
-----------------------------
 
Loss:  1.090424694120884
-----------------------------
 
Loss:  1.087451260536909
-----------------------------
 
Loss:  1.0824933648109436
-----------------------------
 
Loss:  1.07669522985816
-----------------------------
 
Loss:  1.0675967670977116
-----------------------------
 
Loss:  1.0563397780060768
-----------------------------
 
Loss:  1.0437503457069397
-----------------------------
 
Loss:  1.032370701432228
-----------------------------
 
Loss:  1.022795680910349
-----------------------------
 
Loss:  1.0135308839380741
-----------------------------
 
Loss:  1.0069848038256168
-----------------------------
 
Loss:  1.0029426775872707
-----------------------------
 
Loss:  1.000126637518406
-----------------------------
 
Loss:  0.9944429621100426
-----------------------------
 
Loss:  0.9867310523986816
-----------------------------
 
Loss:  0.9766975417733192
-----------------------------
 
Loss:  0.9683619253337383
-----------------------------
 
Loss:  0.9623347781598568
-----------------------------
 
Loss:  0.9583895094692707
-----------------------------
 
Loss:  0.954713299870491
-----------------------------
 
Loss:  0.9495233185589314
-----------------------------
 
Loss:  0.943210069090128
-----------------------------
 
Loss:  0.9361601434648037
-----------------------------
 
Loss:  0.930418074131012
-----------------------------
 
Loss:  0.9244810789823532
-----------------------------
 
Loss:  0.9193023666739464
-----------------------------
 
Loss:  0.9145290590822697
-----------------------------
 
Loss:  0.9096051566302776
-----------------------------
 
Loss:  0.9040310047566891
-----------------------------
 
Loss:  0.898992083966732
-----------------------------
 
Loss:  0.8940846659243107
-----------------------------
 
Loss:  0.8882604539394379
-----------------------------
 
Loss:  0.8833648636937141
-----------------------------
 
Loss:  0.8782817982137203
-----------------------------
 
Loss:  0.8738691918551922
-----------------------------
 
Loss:  0.8684283122420311
-----------------------------
 
Loss:  0.8643929846584797
-----------------------------
 
Loss:  0.8593915030360222
-----------------------------
 
Loss:  0.8543022908270359
-----------------------------
 
Loss:  0.8500554598867893
-----------------------------
 
Loss:  0.8447019383311272
-----------------------------
 
Loss:  0.8407941088080406
-----------------------------
 
Loss:  0.8349691517651081
-----------------------------
 
Loss:  0.831042043864727
-----------------------------
 
Loss:  0.8269927464425564
-----------------------------
 
Loss:  0.8226268924772739
-----------------------------
 
Loss:  0.8175518363714218
-----------------------------
 
Loss:  0.8132521994411945
-----------------------------
 
Loss:  0.8089087903499603
-----------------------------
 
Loss:  0.8040459826588631
-----------------------------
 
Loss:  0.7999412715435028
-----------------------------
 
Loss:  0.7959677837789059
-----------------------------
 
Loss:  0.7912087254226208
-----------------------------
 
Loss:  0.7871444337069988
-----------------------------
 
Loss:  0.7832293398678303
-----------------------------
 
Loss:  0.7783467881381512
-----------------------------
 
Loss:  0.7743659894913435
-----------------------------
 
Loss:  0.7705030031502247
-----------------------------
 
Loss:  0.7658573798835278
-----------------------------
 
Loss:  0.7616705726832151
-----------------------------
 
Loss:  0.7582555990666151
-----------------------------
 
Loss:  0.753592886030674
-----------------------------
 
Loss:  0.7500446867197752
-----------------------------
 
Loss:  0.7455659098923206
-----------------------------
 
Loss:  0.7418923545628786
-----------------------------
 
Loss:  0.7380775175988674
-----------------------------
 
Loss:  0.7342354394495487
-----------------------------
 
Loss:  0.7304895669221878
-----------------------------
 
Loss:  0.726550817489624
-----------------------------
 
Loss:  0.7222803309559822
-----------------------------
 
Loss:  0.7183640263974667
-----------------------------
 
Loss:  0.7143017370253801
-----------------------------
 
Loss:  0.7107905577868223
-----------------------------
 
Loss:  0.7063528057187796
-----------------------------
 
Loss:  0.7027974352240562
-----------------------------
